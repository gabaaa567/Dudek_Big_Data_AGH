{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06474e39-0681-44dd-b61f-3291f937f949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions DataFrame:\n+---------+----------+-------+\n|AccountId|  TranDate|TranAmt|\n+---------+----------+-------+\n|        1|2011-01-01| 500.00|\n|        1|2011-01-15|  50.00|\n|        1|2011-01-22| 250.00|\n|        1|2011-01-24|  75.00|\n|        1|2011-01-26| 125.00|\n|        1|2011-01-28| 175.00|\n|        2|2011-01-01| 500.00|\n|        2|2011-01-15|  50.00|\n|        2|2011-01-22|  25.00|\n|        2|2011-01-23| 125.00|\n|        2|2011-01-26| 200.00|\n|        2|2011-01-29| 250.00|\n|        3|2011-01-01| 500.00|\n|        3|2011-01-15|  50.00|\n|        3|2011-01-22|5000.00|\n|        3|2011-01-25| 550.00|\n|        3|2011-01-27|  95.00|\n|        3|2011-01-30|2500.00|\n+---------+----------+-------+\n\n\nLogical DataFrame:\n+-----+-----------+------+\n|RowID|      FName|Salary|\n+-----+-----------+------+\n|    1|     George|   800|\n|    2|        Sam|   950|\n|    3|      Diane|  1100|\n|    4|   Nicholas|  1250|\n|    5|     Samuel|  1250|\n|    6|   Patricia|  1300|\n|    7|      Brian|  1500|\n|    8|     Thomas|  1600|\n|    9|       Fran|  2450|\n|   10|     Debbie|  2850|\n|   11|       Mark|  2975|\n|   12|      James|  3000|\n|   13|    Cynthia|  3000|\n|   14|Christopher|  5000|\n+-----+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#ZADANIE 3\n",
    "#odpowiednik CREATE TABLE + INSERT\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from decimal import Decimal \n",
    "\n",
    "spark = SparkSession.builder.appName(\"WindowFunctionsDemo\").getOrCreate()\n",
    "\n",
    "transactions_data = [\n",
    "    (1, \"2011-01-01\", Decimal('500.00')),\n",
    "    (1, \"2011-01-15\", Decimal('50.00')),\n",
    "    (1, \"2011-01-22\", Decimal('250.00')),\n",
    "    (1, \"2011-01-24\", Decimal('75.00')),\n",
    "    (1, \"2011-01-26\", Decimal('125.00')),\n",
    "    (1, \"2011-01-28\", Decimal('175.00')),\n",
    "    (2, \"2011-01-01\", Decimal('500.00')),\n",
    "    (2, \"2011-01-15\", Decimal('50.00')),\n",
    "    (2, \"2011-01-22\", Decimal('25.00')),\n",
    "    (2, \"2011-01-23\", Decimal('125.00')),\n",
    "    (2, \"2011-01-26\", Decimal('200.00')),\n",
    "    (2, \"2011-01-29\", Decimal('250.00')),\n",
    "    (3, \"2011-01-01\", Decimal('500.00')),\n",
    "    (3, \"2011-01-15\", Decimal('50.00')),\n",
    "    (3, \"2011-01-22\", Decimal('5000.00')),\n",
    "    (3, \"2011-01-25\", Decimal('550.00')),\n",
    "    (3, \"2011-01-27\", Decimal('95.00')),\n",
    "    (3, \"2011-01-30\", Decimal('2500.00'))\n",
    "]\n",
    "\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"AccountId\", IntegerType()),\n",
    "    StructField(\"TranDate\", StringType()),\n",
    "    StructField(\"TranAmt\", DecimalType(8, 2))\n",
    "])\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions_data, schema=transactions_schema)\n",
    "\n",
    "# konwersja\n",
    "transactions_df = transactions_df.withColumn(\"TranDate\", to_date(col(\"TranDate\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "logical_data = [\n",
    "    (1, \"George\", 800),\n",
    "    (2, \"Sam\", 950),\n",
    "    (3, \"Diane\", 1100),\n",
    "    (4, \"Nicholas\", 1250),\n",
    "    (5, \"Samuel\", 1250),\n",
    "    (6, \"Patricia\", 1300),\n",
    "    (7, \"Brian\", 1500),\n",
    "    (8, \"Thomas\", 1600),\n",
    "    (9, \"Fran\", 2450),\n",
    "    (10, \"Debbie\", 2850),\n",
    "    (11, \"Mark\", 2975),\n",
    "    (12, \"James\", 3000),\n",
    "    (13, \"Cynthia\", 3000),\n",
    "    (14, \"Christopher\", 5000)\n",
    "]\n",
    "\n",
    "logical_schema = StructType([\n",
    "    StructField(\"RowID\", IntegerType()),\n",
    "    StructField(\"FName\", StringType()),\n",
    "    StructField(\"Salary\", IntegerType())\n",
    "])\n",
    "\n",
    "logical_df = spark.createDataFrame(logical_data, schema=logical_schema)\n",
    "\n",
    "#wyniki\n",
    "print(\"Transactions DataFrame:\")\n",
    "transactions_df.show()\n",
    "\n",
    "print(\"\\nLogical DataFrame:\")\n",
    "logical_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f9ca92-7b6f-42db-8ab7-102807fd226d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-----------+\n|AccountId|  TranDate|TranAmt|RunTotalAmt|\n+---------+----------+-------+-----------+\n|        1|2011-01-01| 500.00|     500.00|\n|        1|2011-01-15|  50.00|     550.00|\n|        1|2011-01-22| 250.00|     800.00|\n|        1|2011-01-24|  75.00|     875.00|\n|        1|2011-01-26| 125.00|    1000.00|\n|        1|2011-01-28| 175.00|    1175.00|\n|        2|2011-01-01| 500.00|     500.00|\n|        2|2011-01-15|  50.00|     550.00|\n|        2|2011-01-22|  25.00|     575.00|\n|        2|2011-01-23| 125.00|     700.00|\n|        2|2011-01-26| 200.00|     900.00|\n|        2|2011-01-29| 250.00|    1150.00|\n|        3|2011-01-01| 500.00|     500.00|\n|        3|2011-01-15|  50.00|     550.00|\n|        3|2011-01-22|5000.00|    5550.00|\n|        3|2011-01-25| 550.00|    6100.00|\n|        3|2011-01-27|  95.00|    6195.00|\n|        3|2011-01-30|2500.00|    8695.00|\n+---------+----------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#zapytanie 2\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "window_spec = Window.partitionBy(\"AccountId\").orderBy(\"TranDate\")\n",
    "\n",
    "result_df = transactions_df.withColumn(\n",
    "    \"RunTotalAmt\", \n",
    "    sum(\"TranAmt\").over(window_spec)\n",
    ").orderBy(\"AccountId\", \"TranDate\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8bf7629-d7bd-486e-9e05-7bb35c24037c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+-------+----------+----------+-----------+-----------+-----------+\n|AccountId|  TranDate|month|TranAmt|    RunAvg|RunTranQty|RunSmallAmt|RunLargeAmt|RunTotalAmt|\n+---------+----------+-----+-------+----------+----------+-----------+-----------+-----------+\n|        1|2011-01-01|    1| 500.00|500.000000|         1|     500.00|     500.00|     500.00|\n|        1|2011-02-01|    2|   0.00|250.000000|         2|       0.00|     500.00|     500.00|\n|        1|2011-03-01|    3|   0.00|166.666667|         3|       0.00|     500.00|     500.00|\n|        1|2011-04-01|    4|   0.00|125.000000|         4|       0.00|     500.00|     500.00|\n|        1|2011-05-01|    5|   0.00|100.000000|         5|       0.00|     500.00|     500.00|\n|        1|2011-06-01|    6|   0.00| 83.333333|         6|       0.00|     500.00|     500.00|\n|        1|2011-07-01|    7|   0.00| 71.428571|         7|       0.00|     500.00|     500.00|\n|        1|2011-08-01|    8|   0.00| 62.500000|         8|       0.00|     500.00|     500.00|\n|        1|2011-09-01|    9|   0.00| 55.555556|         9|       0.00|     500.00|     500.00|\n|        1|2011-10-01|   10|   0.00| 50.000000|        10|       0.00|     500.00|     500.00|\n|        1|2011-11-01|   11|   0.00| 45.454545|        11|       0.00|     500.00|     500.00|\n|        1|2011-12-01|   12|   0.00| 41.666667|        12|       0.00|     500.00|     500.00|\n|        2|2011-01-01|    1| 500.00|500.000000|         1|     500.00|     500.00|     500.00|\n|        2|2011-02-01|    2|   0.00|250.000000|         2|       0.00|     500.00|     500.00|\n|        2|2011-03-01|    3|   0.00|166.666667|         3|       0.00|     500.00|     500.00|\n|        2|2011-04-01|    4|   0.00|125.000000|         4|       0.00|     500.00|     500.00|\n|        2|2011-05-01|    5|   0.00|100.000000|         5|       0.00|     500.00|     500.00|\n|        2|2011-06-01|    6|   0.00| 83.333333|         6|       0.00|     500.00|     500.00|\n|        2|2011-07-01|    7|   0.00| 71.428571|         7|       0.00|     500.00|     500.00|\n|        2|2011-08-01|    8|   0.00| 62.500000|         8|       0.00|     500.00|     500.00|\n|        2|2011-09-01|    9|   0.00| 55.555556|         9|       0.00|     500.00|     500.00|\n|        2|2011-10-01|   10|   0.00| 50.000000|        10|       0.00|     500.00|     500.00|\n|        2|2011-11-01|   11|   0.00| 45.454545|        11|       0.00|     500.00|     500.00|\n|        2|2011-12-01|   12|   0.00| 41.666667|        12|       0.00|     500.00|     500.00|\n|        3|2011-01-01|    1| 500.00|500.000000|         1|     500.00|     500.00|     500.00|\n|        3|2011-02-01|    2|   0.00|250.000000|         2|       0.00|     500.00|     500.00|\n|        3|2011-03-01|    3|   0.00|166.666667|         3|       0.00|     500.00|     500.00|\n|        3|2011-04-01|    4|   0.00|125.000000|         4|       0.00|     500.00|     500.00|\n|        3|2011-05-01|    5|   0.00|100.000000|         5|       0.00|     500.00|     500.00|\n|        3|2011-06-01|    6|   0.00| 83.333333|         6|       0.00|     500.00|     500.00|\n|        3|2011-07-01|    7|   0.00| 71.428571|         7|       0.00|     500.00|     500.00|\n|        3|2011-08-01|    8|   0.00| 62.500000|         8|       0.00|     500.00|     500.00|\n|        3|2011-09-01|    9|   0.00| 55.555556|         9|       0.00|     500.00|     500.00|\n|        3|2011-10-01|   10|   0.00| 50.000000|        10|       0.00|     500.00|     500.00|\n|        3|2011-11-01|   11|   0.00| 45.454545|        11|       0.00|     500.00|     500.00|\n|        3|2011-12-01|   12|   0.00| 41.666667|        12|       0.00|     500.00|     500.00|\n+---------+----------+-----+-------+----------+----------+-----------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#zapytanie 3\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# zakres dat\n",
    "all_months = [(account, month) \n",
    "             for account in [1, 2, 3] \n",
    "             for month in range(1, 13)]\n",
    "\n",
    "full_range = spark.createDataFrame(\n",
    "    all_months,\n",
    "    [\"AccountId\", \"month\"]\n",
    ")\n",
    "\n",
    "# kolumna z data\n",
    "full_range = full_range.withColumn(\n",
    "    \"TranDate\",\n",
    "    expr(\"to_date(concat('2011-', month, '-01'))\")\n",
    ")\n",
    "\n",
    "\n",
    "transactions_with_months = full_range.join(\n",
    "    transactions_df,\n",
    "    [\"AccountId\", \"TranDate\"],\n",
    "    \"left\"\n",
    ").fillna(0, subset=[\"TranAmt\"])\n",
    "\n",
    "# f okienkowe \n",
    "window_spec = Window.partitionBy(\"AccountId\").orderBy(\"TranDate\")\n",
    "\n",
    "result = transactions_with_months.withColumn(\n",
    "    \"RunAvg\", avg(\"TranAmt\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"RunTranQty\", count(\"*\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"RunSmallAmt\", min(\"TranAmt\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"RunLargeAmt\", max(\"TranAmt\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"RunTotalAmt\", sum(\"TranAmt\").over(window_spec)\n",
    ").orderBy(\"AccountId\", \"TranDate\")\n",
    "\n",
    "\n",
    "result.show(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dab0608-d0b6-4e2d-8e49-3d4860d65b18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+---+----------+--------+--------+--------+----------+\n|AccountId|  TranDate|TranAmt| RN|  SlideAvg|SlideQty|SlideMin|SlideMax|SlideTotal|\n+---------+----------+-------+---+----------+--------+--------+--------+----------+\n|        1|2011-01-01| 500.00|  1|500.000000|       1|  500.00|  500.00|    500.00|\n|        1|2011-02-01|   0.00|  2|250.000000|       2|    0.00|  500.00|    500.00|\n|        1|2011-03-01|   0.00|  3|166.666667|       3|    0.00|  500.00|    500.00|\n|        1|2011-04-01|   0.00|  4|  0.000000|       3|    0.00|    0.00|      0.00|\n|        1|2011-05-01|   0.00|  5|  0.000000|       3|    0.00|    0.00|      0.00|\n|        1|2011-06-01|   0.00|  6|  0.000000|       3|    0.00|    0.00|      0.00|\n|        1|2011-07-01|   0.00|  7|  0.000000|       3|    0.00|    0.00|      0.00|\n|        1|2011-08-01|   0.00|  8|  0.000000|       3|    0.00|    0.00|      0.00|\n|        1|2011-09-01|   0.00|  9|  0.000000|       3|    0.00|    0.00|      0.00|\n|        1|2011-10-01|   0.00| 10|  0.000000|       3|    0.00|    0.00|      0.00|\n|        1|2011-11-01|   0.00| 11|  0.000000|       3|    0.00|    0.00|      0.00|\n|        1|2011-12-01|   0.00| 12|  0.000000|       3|    0.00|    0.00|      0.00|\n|        2|2011-01-01| 500.00|  1|500.000000|       1|  500.00|  500.00|    500.00|\n|        2|2011-02-01|   0.00|  2|250.000000|       2|    0.00|  500.00|    500.00|\n|        2|2011-03-01|   0.00|  3|166.666667|       3|    0.00|  500.00|    500.00|\n|        2|2011-04-01|   0.00|  4|  0.000000|       3|    0.00|    0.00|      0.00|\n|        2|2011-05-01|   0.00|  5|  0.000000|       3|    0.00|    0.00|      0.00|\n|        2|2011-06-01|   0.00|  6|  0.000000|       3|    0.00|    0.00|      0.00|\n|        2|2011-07-01|   0.00|  7|  0.000000|       3|    0.00|    0.00|      0.00|\n|        2|2011-08-01|   0.00|  8|  0.000000|       3|    0.00|    0.00|      0.00|\n|        2|2011-09-01|   0.00|  9|  0.000000|       3|    0.00|    0.00|      0.00|\n|        2|2011-10-01|   0.00| 10|  0.000000|       3|    0.00|    0.00|      0.00|\n|        2|2011-11-01|   0.00| 11|  0.000000|       3|    0.00|    0.00|      0.00|\n|        2|2011-12-01|   0.00| 12|  0.000000|       3|    0.00|    0.00|      0.00|\n|        3|2011-01-01| 500.00|  1|500.000000|       1|  500.00|  500.00|    500.00|\n|        3|2011-02-01|   0.00|  2|250.000000|       2|    0.00|  500.00|    500.00|\n|        3|2011-03-01|   0.00|  3|166.666667|       3|    0.00|  500.00|    500.00|\n|        3|2011-04-01|   0.00|  4|  0.000000|       3|    0.00|    0.00|      0.00|\n|        3|2011-05-01|   0.00|  5|  0.000000|       3|    0.00|    0.00|      0.00|\n|        3|2011-06-01|   0.00|  6|  0.000000|       3|    0.00|    0.00|      0.00|\n|        3|2011-07-01|   0.00|  7|  0.000000|       3|    0.00|    0.00|      0.00|\n|        3|2011-08-01|   0.00|  8|  0.000000|       3|    0.00|    0.00|      0.00|\n|        3|2011-09-01|   0.00|  9|  0.000000|       3|    0.00|    0.00|      0.00|\n|        3|2011-10-01|   0.00| 10|  0.000000|       3|    0.00|    0.00|      0.00|\n|        3|2011-11-01|   0.00| 11|  0.000000|       3|    0.00|    0.00|      0.00|\n|        3|2011-12-01|   0.00| 12|  0.000000|       3|    0.00|    0.00|      0.00|\n+---------+----------+-------+---+----------+--------+--------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# zapytanie 4\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "months = [(a, f'2011-{m:02d}-01') for a in [1,2,3] for m in range(1,13)]\n",
    "full_df = spark.createDataFrame(months, ['AccountId','TranDate'])\n",
    "\n",
    "transactions_full = full_df.join(transactions_df, ['AccountId','TranDate'], 'left')\\\n",
    "                         .fillna(0, ['TranAmt'])\n",
    "\n",
    "window = Window.partitionBy('AccountId').orderBy('TranDate')\\\n",
    "              .rowsBetween(-2, Window.currentRow)\n",
    "\n",
    "result = transactions_full.withColumn('RN', row_number().over(Window.partitionBy('AccountId').orderBy('TranDate')))\\\n",
    "                         .withColumn('SlideAvg', avg('TranAmt').over(window))\\\n",
    "                         .withColumn('SlideQty', count('*').over(window))\\\n",
    "                         .withColumn('SlideMin', min('TranAmt').over(window))\\\n",
    "                         .withColumn('SlideMax', max('TranAmt').over(window))\\\n",
    "                         .withColumn('SlideTotal', sum('TranAmt').over(window))\\\n",
    "                         .orderBy('AccountId', 'TranDate')\n",
    "\n",
    "result.show(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b112722-04a1-4b32-96d4-bc030f4dc02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------+---------+----------+\n|RowID|      FName|Salary|SumByRows|SumByRange|\n+-----+-----------+------+---------+----------+\n|    1|     George|   800|      800|       800|\n|    2|        Sam|   950|     1750|      1750|\n|    3|      Diane|  1100|     2850|      2850|\n|    4|   Nicholas|  1250|     4100|      5350|\n|    5|     Samuel|  1250|     5350|      5350|\n|    6|   Patricia|  1300|     6650|      6650|\n|    7|      Brian|  1500|     8150|      8150|\n|    8|     Thomas|  1600|     9750|      9750|\n|    9|       Fran|  2450|    12200|     12200|\n|   10|     Debbie|  2850|    15050|     15050|\n|   11|       Mark|  2975|    18025|     18025|\n|   12|      James|  3000|    21025|     24025|\n|   13|    Cynthia|  3000|    24025|     24025|\n|   14|Christopher|  5000|    29025|     29025|\n+-----+-----------+------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#zapytanie 5\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "rows_window = Window.orderBy(\"Salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "range_window = Window.orderBy(\"Salary\").rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "result = logical_df.withColumn(\"SumByRows\", _sum(\"Salary\").over(rows_window)) \\\n",
    "                  .withColumn(\"SumByRange\", _sum(\"Salary\").over(range_window)) \\\n",
    "                  .orderBy(\"RowID\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb26f8f-57c0-465c-a1e0-a7e777a2276f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------+------+----------+----------+-----------+----------+\n|RowID|      FName|Salary|RowNum|NextSalary|PrevSalary|FirstSalary|LastSalary|\n+-----+-----------+------+------+----------+----------+-----------+----------+\n|    1|     George|   800|     1|       950|      null|        800|      5000|\n|    2|        Sam|   950|     2|      1100|       800|        800|      5000|\n|    3|      Diane|  1100|     3|      1250|       950|        800|      5000|\n|    4|   Nicholas|  1250|     4|      1250|      1100|        800|      5000|\n|    5|     Samuel|  1250|     5|      1300|      1250|        800|      5000|\n|    6|   Patricia|  1300|     6|      1500|      1250|        800|      5000|\n|    7|      Brian|  1500|     7|      1600|      1300|        800|      5000|\n|    8|     Thomas|  1600|     8|      2450|      1500|        800|      5000|\n|    9|       Fran|  2450|     9|      2850|      1600|        800|      5000|\n|   10|     Debbie|  2850|    10|      2975|      2450|        800|      5000|\n|   11|       Mark|  2975|    11|      3000|      2850|        800|      5000|\n|   12|      James|  3000|    12|      3000|      2975|        800|      5000|\n|   13|    Cynthia|  3000|    13|      5000|      3000|        800|      5000|\n|   14|Christopher|  5000|    14|      null|      3000|        800|      5000|\n+-----+-----------+------+------+----------+----------+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#funkcji okienkowych\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Definiujemy okno\n",
    "window_spec = Window.orderBy(\"Salary\")\n",
    "\n",
    "# Obliczenia z użyciem funkcji okienkowych\n",
    "result = logical_df.select(\n",
    "    \"RowID\",\n",
    "    \"FName\",\n",
    "    \"Salary\", #expr sluzy do wykonania ezposrednich wyrazen SQL\n",
    "    expr(\"ROW_NUMBER() OVER (ORDER BY Salary)\").alias(\"RowNum\"), #przypisuje numery wierszom kolejne w kazdej partycji\n",
    "    expr(\"LEAD(Salary, 1) OVER (ORDER BY Salary)\").alias(\"NextSalary\"), #zwraca wartosc z kolumny z nastepnego wiersza\n",
    "    expr(\"LAG(Salary, 1) OVER (ORDER BY Salary)\").alias(\"PrevSalary\"), #zwraca warstosc z kolumny z poprzedniego wiersza \n",
    "    expr(\"FIRST_VALUE(Salary) OVER (ORDER BY Salary)\").alias(\"FirstSalary\"), # zwraca pierwsza wartosc z kolumny\n",
    "    expr(\"LAST_VALUE(Salary) OVER (ORDER BY Salary ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\").alias(\"LastSalary\") #ostatnia val col\n",
    ").orderBy(\"RowID\")\n",
    "\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab4_v2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}